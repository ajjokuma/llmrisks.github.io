# Wednesday, 1 Nov: Data Matters: Investigating the Impact of Data on Large Language Models

## Background

This class was on the impact of data on the large language models.

<center>
<div class="slide">
  <img src="../images/week10/Picture1.png" alt="" width="70%">
</div>
</center>

Information exists in diverse formats such as text, images, graphs, time-series data, and more. In this era of information overload, it is crucial to understand the effects of this data on language models. The research have been predominantly concentrating on examining the influences of model and data sizes.

## Content
<center>
<div class="slide">
  <img src="../images/week10/Picture2.png" alt="" width="70%">
</div>
</center>

We are going to look at two important research questions using two of the research papers:
 (1) What types of data are beneficial to the models? – Using the research paper, LIMA: Less is more for Alignment
(2) What are the consequences of low-quality data for the models? – Using the research paper, The Curse of Recursion: Training on Generated Data Makes Models Forget
## LIMA: Less Is More for Alignment
<center>
<div class="slide">
  <img src="../images/week10/Picture3.png" alt="" width="70%">
</div>
</center>

Superficial Alignment Hypothesis - The model learns knowledge and capabilities entirely during pre-training phase, while alignment teaches about the specific sub-distribution of formats to be used or styles to be used when interacting with the users.

## Approach

<center>
<div class="slide">
  <img src="../images/week10/Picture4.png" alt="" width="70%">
</div>
</center>


The authors curated a diverse set of training prompts, selecting 750 top questions from community forums and complementing them with 250 manually crafted examples of prompts and responses. They prioritized quality, diversity, and uniformity in their selection process, all while utilizing a smaller dataset. To evaluate performance, LIMA was benchmarked against state-of-the-art language models using a set of 300 challenging test prompts.
## Findings

<center>
<div class="slide">
  <img src="../images/week10/Picture5.png" alt="" width="70%">
</div>
</center>

The initial figure showcases the outcomes of a human-based evaluation, comparing LIMA’s performance across various model sizes. Meanwhile, the second figure employs GPT-4 for evaluation purposes. In the comparisons with the first two language models, LIMA emerges as the victor. Notably, it’s interesting to observe that LIMA secures a 19% win rate against GPT-4, even when GPT-4 itself is utilized as the evaluator.
<center>
<div class="slide">
  <img src="../images/week10/Picture6.png" alt="" width="70%">
</div>
</center>

They additionally evaluated LIMA's performance in terms of out-of-distribution scenarios and robustness. LIMA safely addressed 80% of the sensitive prompts presented to it. Regarding responses to out-of-distribution inputs, LIMA performed exceptionally well.
<center>
<div class="slide">
  <img src="../images/week10/Picture7.png" alt="" width="70%">
</div>
</center>

In the concluding series of experiments, the research team delved into how the diversity of the test data influences LIMA. They observed that the filtered Stack Exchange dataset exhibited both diversity and high-quality responses. In contrast, the unfiltered Stack Exchange dataset, while diverse, lacked in quality. On the other hand, wikiHow provided high-quality responses specifically to “how to” prompts. LIMA showcased impressive performance when dealing with datasets that were both highly diverse and of high quality. Additionally, the authors explored the effects of data volume, with their findings illustrated in Figure 6. Interestingly, they noted that the quantity of data did not exert any noticeable impact on the quality of the generated content.
## Personal Thought
<center>
<div class="slide">
  <img src="../images/week10/Picture8.png" alt="" width="70%">
</div>
</center>

This slide delves into the personal perspectives of the presenter on drawing parallels between Large Language Models (LLMs) and students, as well as evaluating the influence of LIMA on a student’s learning journey.
The take-away message from the first paper was that quality and diversity of the data is more important for LLM than the quantity of the data.
## The Curse of Recursion: Training on Generated Data Makes Models Forget – Motivation

<center>
<div class="slide">
  <img src="../images/week10/Picture9.png" alt="" width="70%">
</div>
</center>

The second paper is driven by the aim to scrutinize the effects from employing training data generated by preceding versions of GPT, such as GPT-(n-1), GPT-(n-2), and so forth, in the training process of GPT-(n). 
## Model Collapse
<center>
<div class="slide">
  <img src="../images/week10/Picture10.png" alt="" width="70%">
</div>
</center>

This paper discusses about model collapse, which is a degenerative process where the model no longer remembers the underlying true distribution. This mainly happens with data where the probability of occurrence is low.
<center>
<div class="slide">
  <img src="../images/week10/Picture11.png" alt="" width="70%">
</div>
</center>

The two main causes for the model collapse are: (i) Statistical approximation error that occurs as the number of samples are finite, and (ii) Functional approximation error that occurs due to the function approximators being not expressive enough.
<center>
<div class="slide">
  <img src="../images/week10/Picture12.png" alt="" width="70%">
</div>
</center>

Here, the model collapse is expressed through mathematical terms, providing a comprehensive overview of the feedback mechanism involved in the learning process. Initially, the data are presumed to be meticulously curated by humans, ensuring a pristine starting point. Following this, Model 0 undergoes training, and subsequently, data are sampled from it. At stage n in the process, the data collected from step n - 1 are incorporated into the overall dataset. This comprehensive dataset is then utilized to train Model n. Ideally, when Monte Carlo sampling is employed to obtain data, the resultant dataset should statistically align closely with the original, assuming that the fitting and sampling procedures are executed flawlessly. This entire procedure mirrors the real-world scenario witnessed on the Internet, where data generated by models become increasingly prevalent and integrated into subsequent learning and development cycles. This creates a continuous feedback loop, where model-generated data are perpetually fed back into the system, influencing future iterations and their capabilities.

The following four slides delve into a theoretical analysis of both discrete and continuous distributions.
Discrete Distribution:
In scenarios where a discrete distribution is under consideration, events with low probabilities tend to degenerate when the sample size is finite. As the number of time steps advances towards infinity, the distribution converges towards a delta distribution. This phenomenon results in the preservation of only the events with high probabilities, while those less likely start to fade away.
Continuous Distribution:
Assuming the initial distribution to be a Gaussian distribution, the distribution of Xji is analyzed. The resulting distribution follows a variance-gamma distribution, showcasing a divergence from the initial Gaussian form. When Mi = M, the variance’s difference exhibits a linear growth with respect to n. This indicates that as we proceed through time steps or iterations, the divergence from the initial distribution becomes more pronounced. To quantify the exact extent of divergence between distributions, the Wasserstein-2 distance is employed. This measure provides a precise way to understand how far apart the distributions have moved from each other. The analysis reveals that in order to maintain a finite distance as indicated by the Wasserstein-2 measure, Mi must increase at a rate that is super-linear with respect to n.

<center>
<div class="slide">
  <img src="../images/week10/Picture13.png" alt="" width="70%">
</div>
</center>

<center>
<div class="slide">
  <img src="../images/week10/Picture14.png" alt="" width="70%">
</div>
</center>

<center>
<div class="slide">
  <img src="../images/week10/Picture15.png" alt="" width="70%">
</div>
</center>

<center>
<div class="slide">
  <img src="../images/week10/Picture16.png" alt="" width="70%">
</div>
</center>


