+++
date = "21 Aug 2023"
draft = false
title = "Readings and Topics"
slug = "readings"
+++

This page collects some potential topics and readings for the seminar.

## Introduction (Week 1)

[Introduction to Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/introduction/) (from Stanford course)

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. [_Attention Is All You Need_](https://arxiv.org/abs/1706.03762). [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762). NeurIPS 2017.


Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding_](https://aclanthology.org/N19-1423/). ACL 2019.


(optional) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. [_Language Models are Unsupervised Multitask Learners_](/docs/language-models.pdf). OpenAI, 2019.

These two blog posts by Jay Alammar are helpful for understanding attention and Transformers:

- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa
Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton,
Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne
Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason
Gabriel. [_Ethical and social risks of harm from Language
Models_](https://arxiv.org/abs/2112.04359) DeepMind, 2021. [https://arxiv.org/abs/2112.04359](https://arxiv.org/abs/2112.04359)

Simon Willison. [_Catching up on the weird world of LLMs_](https://simonwillison.net/2023/Aug/3/weird-world-of-llms/), August 2023.

### Viewpoint Essays

Douglas Hofstadter. [_Gödel, Escher, Bach, and AI_](https://archive.is/2VtiC). The Atlantic. 8 July 2023.

Marc Andreessen. [AI Will Save the World](https://www.thefp.com/p/why-ai-will-save-the-world). 11 July 2023

Paul Kingsnorth. [_Rage Against the Machine_](https://www.thefp.com/p/rage-against-the-machine-ai-paul-kingsnorth). 12 July 2023

### Broad Overviews

Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy. [_Challenges and Applications of Large Language Models_](https://arxiv.org/abs/2307.10169). [https://arxiv.org/abs/2307.10169](https://arxiv.org/abs/2307.10169)

Center for Research on Foundation Models (CRFM) at the Stanford
Institute for Human-Centered Artificial Intelligence (HAI). [_On the
Opportunities and Risks of Foundation
Models_](https://arxiv.org/abs/2108.07258) [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258). [Report Page](https://crfm.stanford.edu/report.html)


## Copyright and Law

Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. [_AI and Law: The Next Generation_](https://genlaw.github.io/explainers/explainers.pdf). July 2023. [https://genlaw.github.io/explainers/](https://genlaw.github.io/explainers/)

Inyoung Cheong, Aylin Caliskan, Tadayoshi Kohno. [_Is the U.S. Legal System Ready for AI's Challenges to Human Values?_](https://arxiv.org/abs/2308.15906). 30 August 2023.

[Sarah Silverman, Christopher Golden, and Richard Kadrey vs. OpenAI](https://s3.documentcloud.org/documents/23869693/silverman-openai-complaint.pdf). Legal Complaint against ChatGPT, file 7 July 2023.

Nikhil Vyas, Sham Kakade, Boaz Barak. [_On Provable Copyright Protection for Generative Models_](https://arxiv.org/abs/2302.10870). [https://arxiv.org/abs/2302.10870](https://arxiv.org/abs/2302.10870).

Federal Register. [_Artificial Intelligence and Copyright_](https://www.federalregister.gov/documents/2023/08/30/2023-18624/artificial-intelligence-and-copyright). Request for Comment (Published 30 August 2023, Comments due by 15 November 2023)

## Governance and Regulation

Michael Veale, Kira Matus, Robert Gorwa. [_AI and Global Governance: Modalities, Rationales, Tensions_](https://discovery.ucl.ac.uk/id/eprint/10171121/1/Veale%20Matus%20Gorwa%202023.pdf). Annual Review of Law and Social Science, 2023.

## Amplification Techniques

Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awada. [_Orca: Progressive Learning from Complex Explanation Traces of GPT-4_](https://arxiv.org/abs/2306.02707). [https://arxiv.org/abs/2306.02707](https://arxiv.org/abs/2306.02707)

## Programming with LLMs

## Performance of LLMs

Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin,
Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan
Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas
Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman. [_Studying
Large Language Model Generalization with Influence
Functions_](https://arxiv.org/abs/2308.03296). [https://arxiv.org/abs/2308.03296](https://arxiv.org/abs/2308.03296)

## Integrations

Ernest Davis, Scott Aaronson. [_Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems_](https://arxiv.org/abs/2308.05713). [https://arxiv.org/abs/2308.05713](https://arxiv.org/abs/2308.05713)



## Hallucination

Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez. [_Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations_](https://arxiv.org/abs/1703.03717). IJCAI 2017.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch. [_Improving Factuality and Reasoning in Language Models through Multiagent Debate_](https://arxiv.org/abs/2305.14325) [https://arxiv.org/abs/2305.14325](https://arxiv.org/abs/2305.14325).

## Abuses of LLMs

Nicholas Carlini. [_A LLM Assisted Exploitation of AI-Guardian_](https://arxiv.org/abs/2307.15008). 

Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson. [_Universal and Transferable Adversarial Attacks on Aligned Language Models_](https://arxiv.org/abs/2307.15043). [https://arxiv.org/abs/2307.15043](https://arxiv.org/abs/2307.15043).
[Project Website: https://llm-attacks.org/](https://llm-attacks.org/).

Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz. [_Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection_](https://arxiv.org/abs/2302.12173). [https://arxiv.org/abs/2302.12173](https://arxiv.org/abs/2302.12173).


## Fairness and Bias

Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov. [_From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models_](https://arxiv.org/abs/2305.08283). ACL 2023.

Myra Cheng, Esin Durmus, Dan Jurafsky. [_Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models_](https://arxiv.org/abs/2305.18189). ACL 2023.


## "Alignment"

Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li. [_Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment_](https://arxiv.org/abs/2308.05374). [https://arxiv.org/abs/2308.05374](https://arxiv.org/abs/2308.05374).


## AGI

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang. [_Sparks of Artificial General Intelligence: Early experiments with GPT-4_](https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/). Microsoft, March 2023. [https://arxiv.org/abs/2303.12712](https://arxiv.org/abs/2303.12712)

Yejin Choi. [_The Curious Case of Commonsense Intelligence_](https://www.amacad.org/publication/curious-case-commonsense-intelligence). Daedalus, Spring 2022.

Konstantine Arkoudas. [_GPT-4 Can't Reason_](https://arxiv.org/abs/2308.03762). [https://arxiv.org/abs/2308.03762](https://arxiv.org/abs/2308.03762).

Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz. [_Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models_](https://arxiv.org/abs/2305.14763). 

Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia
Tsvetkov. [_Minding Language Models' (Lack of) Theory of Mind: A
Plug-and-Play Multi-Character Belief
Tracker_](https://arxiv.org/abs/2306.00924). ACL 2023

Boaz Barak. [_The shape of AGI: Cartoons and back of envelope_](https://windowsontheory.org/2023/07/17/the-shape-of-agi-cartoons-and-back-of-envelope/). July 2023.


## Art

## Writing

## Human Dignity and Job Loss

## "Cheating"

## Scaling

## Embeddings

## Prompt Engineering and "Jailbreaking"

Alexander Wei, Nika Haghtalab, Jacob Steinhardt. [_Jailbroken: How Does LLM Safety Training Fail?_](https://arxiv.org/abs/2307.02483). July 2023.



## "Safety"

## Sentience

Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias Michel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, Rufin VanRullen. [_Consciousness in Artificial Intelligence: Insights from the Science of Consciousness_](https://arxiv.org/abs/2308.08708). August 2023.

## Memorization and Inference Privacy

R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz. [_How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN_](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00567/116616/How-Much-Do-Language-Models-Copy-From-Their). TACL 2023. 


## Preventing Learning

Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben
Y. Zhao. [_Glaze: Protecting Artists from Style Mimicry by
Text-to-Image
Models_](http://people.cs.uchicago.edu/~ravenben/publications/pdf/glaze-usenix23.pdf). USENIX Security 2023.  [Glaze Project Website](https://glaze.cs.uchicago.edu/) [https://arxiv.org/abs/2302.04222](https://arxiv.org/abs/2302.04222)

Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom
Goldstein. [_What Can We Learn from Unlearnable
Datasets?_](https://arxiv.org/abs/2305.19254). [https://arxiv.org/abs/2305.19254](https://arxiv.org/abs/2305.19254)

Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros
G. Dimakis, Adam Klivans.  [_Ambient Diffusion: Learning Clean
Distributions from Corrupted Data_](https://arxiv.org/abs/2305.19256). [https://arxiv.org/abs/2305.19256](https://arxiv.org/abs/2305.19256)

## Training on Generated Data

Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson. [_The Curse of Recursion: Training on Generated Data Makes Models Forget_](https://arxiv.org/abs/2305.17493). [https://arxiv.org/abs/2305.17493](https://arxiv.org/abs/2305.17493)

Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk. [_Self-Consuming Generative Models Go MAD_](https://arxiv.org/abs/2307.01850). [https://arxiv.org/abs/2307.01850](https://arxiv.org/abs/2307.01850).

## Environmental Harms

## Evaluating LLMs

Percy Liang, et al. [_Holistic Evaluation of Language Models_](https://arxiv.org/abs/2211.09110). [HELM Project](https://crfm.stanford.edu/helm/v0.2.2/).

## Useful Guides

[_How to Use AI to Do Stuff: An Opinionated Guide_](https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated) (Ethan Mollick)

[OpenAI Cookbook](https://github.com/openai/openai-cookbook)


### More Sources

[Awesome-LLM: a curated list of Large Language Model Papers and Links](https://github.com/Hannibal046/Awesome-LLM)

[LLM Security](https://llmsecurity.net/) &mdash; collection of papers on LLM security

[COS 597G (Fall 2022): Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/) (Princeton Course taught by [Danqi Chen](https://www.cs.princeton.edu/~danqic/)

[CS324 - Large Language Models (Winter 2022)](https://stanford-cs324.github.io/winter2022/) (Stanford Course taught by [Percy Liang](https://cs.stanford.edu/~pliang/), [
Tatsunori Hashimoto](https://thashim.github.io/), and [Christopher Ré](https://cs.stanford.edu/~chrismre/))